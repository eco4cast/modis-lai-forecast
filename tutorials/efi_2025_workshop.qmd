---
title: "EFI_2025_Workshop"
author: "John Smith, Will Hammond, Chris Jones, David Durden"
date: "2025-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1 About this tutorial

## 1.1 Learning objectives

<!-- -   Overview of the [Post Fire Recovery - MODIS LAI](https://projects.ecoforecast.org/neon4cast-docs/) -->
<!--     theme for the [NEON Ecological Forecast Challenge](https://projects.ecoforecast.org/neon4cast-ci/) -->
-   How to download the necessary data from Planetary Computer
-   How to create a simple forecast for the Post Fire Recovery theme
-   How to submit/score a forecast to evaluate its accuracy


## 1.2 Target user groups for this tutorial

This tutorial is intended to be used by ecological forecasters at any
stage of expertise and may be used as a learning tool as an introduction
to forecasting properties of ecological processes spatially and temporally.
Below, we provide code for introductory examples to walk through the
entire process of creating and submitting a forecast to the Ecological 
Forecasting challenge. This includes:

1.  Accessing target datasets of MODIS LAI.
2.  Accessing climate forecast data to use as drivers in models
    predicting LAI recovery post fire.
3.  How to create models for raster data.
4.  How to submit a forecast to the forecast challenge

Upon completing this tutorial, participants should be able to create and
submit forecasts to the MODIS LAI Post Fire Recovery theme of the EFI RCN NEON
Ecological Forecasting challenge.

## 1.3 Things you will need to complete this tutorial

You will need a current version of R (v4.4 or newer) to complete this
tutorial. We also recommend the RStudio IDE to work with R.

To complete the workshop via this markdown document the following
packages will need to be installed:

-   `tidyverse`
-   `lubridate`
-   `tsibble`
-   `stars`
-   `gdalcubes`
-   `rstac`
-   `terra`
-   `scoringRules`
-   `forecast`
-   `nimble`
-   `spatstat.sparse`
-   `assertthat`
-   `minioclient`
-   `purrr`
-   `remotes` (to install neon4cast from gitHub)
-   `neon4cast` (from github)

The following code chunk should be run to install packages.

``` r
install.packages('tidyverse') # collection of R packages for data manipulation, analysis, and visualisation
install.packages('lubridate') # working with dates and times
install.packages('stars') 
install.packages('gdalcubes') 
install.packages('rstac') 
install.packages('terra')
install.packages('scoringRules') # package to score forecasts
install.packages('forecas')
install.packages('nimble')
install.packages('spatstat.sparse')
install.packages('assertthat')
install.packages('minioclient')
install.packages('purrr')
install.packages('remotes')
remotes::install_github('eco4cast/neon4cast') # package from NEON4cast challenge organisers to assist with forecast building and submission
remotes::install_github("eco4cast/score4cast") # package to score forecasts
```

Then load the packages.

``` r
version$version.string
```

    ## [1] "R version 4.3.1 (2023-06-16 ucrt)"

``` r
library(tidyverse)
library(lubridate)
library(tsibble)
library(stars)
library(gdalcubes)
library(rstac)
library(terra)
library(scoringRules)
library(forecast)
library(nimble)
library(spatstat.sparse)
library(assertthat)
library(minioclient)
library(purrr)
library(remotes)
library(neon4cast)
```

# 2 Introduction

## 2.1 [The NEON Ecological Forecast Challenge](https://projects.ecoforecast.org/neon4cast-ci/)

The Challenge has been organized by the Ecological Forecasting
Initiative Research Coordination Network ([EFI RCN](https://ecoforecast.org/)). The EFI-NEON Forecasting Challenges are the first to set up a system to continually compile and score community generated forecasts for true out of sample data prior to data collection. *Some additional comments about the temporal challenge here*. Questions about ecosystems and ecosystem services are inherently focused on the future because they involve understanding how natural systems will respond to changing environmental, social, and economic conditions over time. Commonly ecosystems are analyzed as single longitudinal time series. However, the intrinsic biodiversity within ecosystems leads to spatial gradients with varying ecosystem services. In this workshop, we present our prototype of a spatio-temporal forecasting challenge. This project represents a novel direction for the EFI-RCN and the field of ecological forecasting as a whole, by moving from single-site temporal forecast models to the consideration of entire eco-regions in a spatio-temporal setting.

_The Challenge asks the scientific community to produce ecological forecasts of future observations of ecological data that will be collected and published by the [National Ecological Observatory Network (NEON)](https://www.neonscience.org/). The Challenge is split into [five themes](https://projects.ecoforecast.org/neon4cast-ci/targets.html#sec-starting-sites) that span aquatic and terrestrial systems, and population, community, and ecosystem processes across a broad range of ecoregions. We are excited to use this Challenge to learn more about the predictability of ecological processes by forecasting NEON data before it is collected._

While we believe that it is crucial to incorporate spatial components into existing temporal forecasting models, there are challenges to address including the evaluation of spatially misaligned forecasts, increased model complexity and computation time, and the need to archive metadata for spatio-temporal ensemble forecasts. There are also natural research questions that scaffold from all of the previous research done in the context of the EFI-NEON forecasting challenge: are the same modeling frameworks that we deemed successful in this forecasting challenge well-suited to the addition of a spatial component? Are these models still successful but difficult to scale, computationally? Does the additional of granular spatial covariates help us to improve predictive performance of the model over it's temporal counterpart? All of these questions are wide open but we need **you**, the Ecological Forecasting community, to help! 

_The Challenge is open to any individual or team from anywhere around the world that wants to submit forecasts. Learn more about how you can participate [here.](https://projects.ecoforecast.org/neon4cast-ci/instructions.html)._

## 2.2 Goals for forecasts of post fire recovery

Ecologist are interested in tracking how landscapes and ecosystems **recover 
from disturbances such as fire** (changes in leaf area index (LAI)). The timing and pace of 
recovery can depend on many factors such as fire severity, ecosystem type, weather,
species present in the location, and species present around the location. For this 
challenge we are forecasting MODIS LAI averaged across the month to account for
cloud cover and other factors. The MODIS LAI product is 500 m spatial resolution
and as such LAI values for each pixel are the combination of the combined 
foliage of all individuals and species in that 500 m pixel. Each fire
that is being forecast has multiple pixels that need to be forecast, so there are 
multiple scores for each fire for each time step.


The speed and trajectory of post fire recovery varies for many reasons and it is 
important to understand the drivers of post fire recovery in LAI. By knowing how
different ecosystem recovery post-fire over time we can forecast recovery in 
LAI for different ecosystems post fire. These forecasts could be used to help 
determine management of various ecosystems under future climates.

There are many open questions that this challenge could help us address:
- What models perform the best for predicting LAI recovery post fire? and do they change by ecosystem type?
- What are the key drivers for post fire recovery and do they change across ecosystem types or fire severity?
- Are areas within each fire more predictable than others? If so why?

There are many more open questions and we will have a survey at the end to gather 
the ones that everyone is interested in and to help us guide the challenge.

## 2.3 Overview of the [MODIS LAI Post Fire Recovery](https://projects.ecoforecast.org/neon4cast-docs/) theme

**What**: Forecast Leaf Area Index (LAI) recovery post fire based on MODIS LAI. 
More information on the MODIS LAI data product (MOD15A2H.061: Terra Leaf Area Index/FPAR 8-Day Global 500m)
we are forecasting can be found
[here](https://lpdaac.usgs.gov/products/myd15a2hv061/) or [here](https://planetarycomputer.microsoft.com/dataset/modis-15A2H-061#overview). 
Note: We are not downloading the target datasets from the Planetary computer but 
are downloading a version that has had images with cloud cover greater than 10%
removed and then the MODIS LAI values averaged for the each month instead of the
8 days of the MODIS LAI product.

-   `LAI`: Leaf Area Index in each cell within the fire bounding box.


**Where**: 1 Fire selected from the [Fire Database](https://www.neonscience.org/field-sites/explore-field-sites). The august
complex fire is the one we are focusing on for this workshop.

**When**: Target data are available as early as 2010 across our selected fires 
for monthly averages in LAI for each cell for each fire. Forecasts are accepted
up to 3 months ahead on a monthly time step.w

# 3 Forecasting MODIS LAI Recovery Post Fire

## 3.1 Define spatial and temporal parameters for our forecast


## 3.2 Read in the data


## 3.3 Visualise the target data


## 3.4 Create the training dataset



## 3.5 Example forecasts: some simple models

The main branch currently has two baseline models implemented. These models are intended to provide a benchmark for evaluating the performance of more complex models. The two baseline models that are currently implemented are:

- A spatial climatology ensemble model
- A grid ARIMA parametric model

We also have two baseline models in the development and debugging stage:
- A spatial-temporal ICAR random walk
- A spatial Gaussian Process 

For now, the first two models that we have implemented will provide reasonable competitors to the models developed by challenge participants, and in the context of this workshop will provide a nice demonstration of the difference between parametric and ensemble models and how they are scored.

### 3.5.1 Forecast MODIS LAI post fire: Local Climatology Model

The first baseline model is a *local climatology* model. Not to be confused with a climate model (which many participants may be familiar with), the purpose of a climatology model is to use historical data to build a forecast distribution.

The *local* part of the climatology model comes into play when we consider exactly what historical data is used to create the forecast distribution. As an example, suppose that we have ten years of daily temperature data for the entire state of Virginia. If our end goal is to create a forecast distribution for the temperature in Blacksburg, we may only want to consider the historical data from Montgomery County instead of the entire state. Indeed, the term *local* refers to the fact that we are only considering points that are, in some broad sense, close to our target in space or time.

In the context of generating baseline forecasts for post-fire LAI recovery, we have data from MODIS dating back to 2002 that comes in roughly every 8 days. Rather than using the entire dataset, we consider only LAI measurements that occur in the month of interest. This gives us a snapshot of historical data within a given month so that we can build an informed forecast distribution for how we would expect our process to behave. For example, if we are interested in forecasting the LAI at the Soaproot Saddle NEON site for May 2025, the local climatology model will include LAI measurements from May 2002, May 2003, etc all the way until May 2024.

A natural follow-up question: once we have identified the data, we want a baseline model. So what exactly is the statistical model that we are using here?

```{r, echo = FALSE}
set.seed(05192025)
dat <- c(rnorm(10, mean = 4), rnorm(10, mean = 10)) 
hist(dat, breaks = 200, ylim = c(0, 2),
     main = 'Histogram of 20 Data Points')
```

In fact these singleton data points *are* our statistical model, at least after a little bit of work. We put some little densities around them (sometimes called "kernel dressing" in the literature) and these little densities come together to turn these singleton points into a forecast distribution. Taking our example from above, this process yields the following:

```{r}
plot(density(dat))
```

By choosing to "fit" a non-parametric model to our data we get some additional flexibility, since we don't have to conform to some particular pre-specified probability distribution. For example, if we were to just take a mean and standard deviation of the above data and treat it as a univariate Normal distribution we would be missing out on the fact that our data is actually bimodal with two areas of high density.

In many situations, the climatology or local climatology models perform surprisingly well. To paraphrase Benjamin Franklin: "death, taxes, and losing to the climatology". Post fire LAI recovery presents an interesting challenge for historical climatology data however: sites that not previously had a fire during the MODIS period from 2002 to present may not adequately capture the behavior of LAI during post fire-recovery. 

Now, for the interesting part, a demonstration of how to fit the climatology model! The function `spat_climatology()` builds a climatology forecast using historical data for a given month, and stores an ensemble of `geotiff` files. In the event that there are missing historical data for a given month, missing values are imputed using a simple bootstrap re-sample of previous values within a pixel (using the internal `na_bootstrap()` function). For cyberinfrastructure pipeline purposes, `spat_climatology` returns the directory that ensemble forecasts were written to.

```{r, echo = FALSE}
for (f in list.files(here::here("R"), full.names = TRUE)) source (f)
library(tidyverse)
suppressPackageStartupMessages(source("../packages.R"))
```

```{r}
fire_box <- fire_bbox(fire = "august_complex", pad_box = TRUE, dir = '../shp')
```

```{r, eval = TRUE}
# Ingest data ------------------------------------------------------------
gdalcubes::gdalcubes_options(parallel=TRUE)

# use ingest_planetary_data function to extract raster cube for fire bounding box between Jan 1 2002 and July 1 2023.
raster_cube <- ingest_planetary_data(start_date = "2015-01-01", 
                                     end_date = "2025-03-01", 
                                     box = fire_box$bbox,
                                     srs = "EPSG:4326",
                                     dx = 0.1, 
                                     dy = 0.1, 
                                     dt = "P30D",
                                     collection = "modis-15A2H-061",
                                     asset_name = "Lai_500m")
```

```{r, include=FALSE}
# Forecast ----------------------------------------------------------------
library(minioclient)
ensemble_forecast_dir <- spat_climatology(cuberast = raster_cube,
                                          date = '2025-03-01',
                                          dir = 'climatology')

install_mc()
mc_alias_set("efi", "data.ecoforecast.org", access_key = "", secret_key = "")

ensemble_forecast_dir |>
  spat4cast_submit(model_id = 'johnsmith')
```

### 3.5.2 Forecast MODIS LAI post fire: gridded ARIMA model

The Grid ARIMA baseline model takes each grid cell in the fire polygon and fits a Seasonal Autoregressive Integrated Moving Average (SARIMA) model. This is a classical technique from time series analysis that combines differencing with autoregressive and moving average models.

If we are in the business of providing technical definitions: a time series $x = \{x_1, ..., x_T \}$ follows a **SARIMA(p, d, q)(P, D, Q)$_s$** process if it can be written in the form:
\begin{align*}
\phi_P (B^s) \Phi_p (B) (1 - B^s)^D (1 - B)^d x_t = \theta_q (B^s) \Theta_q (B) w_t
\end{align*}
(do we need this? can I just link to other SARIMA material?). For our purposes, though, we just need to think of the SARIMA model as a flexible time series technique that lets us capture the temporal autocorrelation, seasonal effects, and correlated errors within a grid cell in a statistically cohesive manner.

There are a number of quantities that require estimation within each grid cell. The actual specification of each $p_{i,j}, q_{i,j}, d_{i,j}$ as well as estimates of their associated parameters $\vec{\theta}_{i,j}, \vec{\phi}_{i,j}, \sigma^2_{i,j}$ are computed using the `auto.arima()` function in the `forecast` library. We do this by taking the time series of LAI data within a particular grid cell, log transforming it, and letting `auto.arima()` pick the best model. We then generate forecasts for the next time point(s) using `S3` method for the `forecast` function and extract the mean and standard deviation. 

We fit our grid cell SARIMA baseline model using the `spat_arima_grid()` function. `spat_arima_grid` takes a matrix as input (we might want to change this to take the target file?) and fits a SARIMA model using `auto.arima` to each grid cell. Grid cells with less than 25 available data points are not considered. As with `spat_climatology`, the `spat_arima_grid` function returns the directory that forecasts were written to. 

```{r, eval = F}
target_mat <- read_csv('../nimble_mat.csv')
parametric_forecast_dir1 <- spat_arima_grid(cuberast = target_mat,
                                          date = '2025-03-01',
                                          dir = 'gsarima')

parametric_forecast_dir1 |>
  spat4cast_submit(model_id = 'johnsmith')

```

It is likely that grid cells that are close to each other exhibit spatial autocorrelation, and thus it is foolish to throw away that valuable information by treating each grid cell as independent. However, this model is meant to be a simple baseline comparator that demonstrates how parametric forecasts are scored. Baseline models that are currently in development seek to account for this spatial and temporal autocorrelation in a statistically rigorous manner. 



Does this forecast seem reasonable?

## 3.6 How to submit a forecast to the NEON Forecast Challenge




# 4 Evaluating your forecast

## 4.1 How your submission will be scored



## 4.2 How to score your own forecast

-   Which model(s) did best?
-   What might explain the differences in scores between the different
    models?
    
## 4.3 Improving your forecasts: Incorporating additional data

An obvious choice for improving on the baseline models presented here is to incorporate covariate information into our forecasts of LAI. Three of our focal fires occurred at NEON sites. This means that we have access to a wealth of covariate information, if we are able to access it and process it! In this section, we provide example code that uses the `neonstore` package to pull and aggregate NEON data for:

- Temperature (daily minimum and maximum temperature)
- Incoming shortwave radiation (daily)
- Relative humidity (daily)

In order to avoid any potential package and dependecy related issues with the installation of `neonstore`, we provide all of the example data from this section in a `.csv` file format on the bucket, so that participants can immediately jump into model building. This process can be roughly broken down into four steps:

- Identifying your site of interest
- Finding and accessing a data product for your covariate of interest
- Aggregating data to the desired time scale
- (Optional) Imputing missing data

```{r, eval = FALSE}
## set site to desired NEON site
## here we will use SOAP, but the other focal
## sites can be used (GRSM or SRER)
site <- 'SOAP'
```

The second step is to find a data product that contains your covariate of interest. While these seems like it may be a daunting task, thanks to NEON's wonderful data portal (found [here]: (https://data.neonscience.org/)) it is not difficult at all. Searching "air temperature" in the data portal search bar gives us multiple options: we will go with Triple Aspirated Air Temperature (`DP1.00003.001`).

```{r, eval = F}
## air temp data product
air_temp_dp <- 'DP1.00003.001'
```

Once we have identified our site and data product of interest, we can use the `neon_download` function from the `neonstore` package to pull the data. For further reading on `neonstore`, Claire K. Lunch has an excellent tutorial that can be found [here](https://www.neonscience.org/resources/learning-hub/tutorials/neonstore-stackfromstore-tutorial).

```{r, eval = F}
## use neonstore package "neon_download" function to pull data product
## for triple aspirated air temperature
neon_download(product = air_temp_dp, site = site)
```

With our data product of interest downloaded, we can now focus on aggregation to our desired timescale. Triple aspirated air temperature is available as one and thirty minute averages, and here we demonstrate how to aggregate to daily minimum and maximum temperature.

```{r, eval = FALSE}
## read the 30 minute measurements into temp_table
temp_table <- neon_read("TAAT_30min-basic", site = site)

## create table of minimum and maximum temps
temp_info <- temp_table %>%
  ## use lubridate to extract date format
  ## extract only max and min data that does not
  ## have a quality flag 
  mutate(Date = as_date(endDateTime),
         tempTripleMaximum = if_else(finalQF == 0,
                              tempTripleMaximum,
                              NA),
         tempTripleMinimum = if_else(finalQF == 0,
                              tempTripleMinimum,
                              NA)) %>%
  ## group_by date to get daily mins/maxs
  group_by(Date) %>%
  ## compute minimums and maximums
  summarize(MinT_daily = min(tempTripleMinimum, na.rm = TRUE),
            MaxT_daily = max(tempTripleMaximum, na.rm = TRUE)) %>%
  ## change infinite values to NAs for interpolation
  mutate_if(is.numeric, ~
              replace(., is.infinite(.), NA))
```

The optional fourth step is imputation of missing data. In this example, we provide code for data imputation using a Kalman filter. We do note that data imputation can be done in many different ways, and thus in the example covariate data provided we include columns with both aggregated imputed data and aggregated raw data for any user who would like to employ their own imputation strategies.

```{r, eval = FALSE}
library(imputeTS) ## for na_kalman

## interpolate missing temperature values using a kalman filter
temp_info$MinTInterp <- na_kalman(temp_info$MinT_daily)
temp_info$MaxTInterp <- na_kalman(temp_info$MaxT_daily)
```

Though we won't go through them in detail, additional code for pulling and aggregating shortwave radiation and relative humidity data is below:

```{r, eval = FALSE}
## set data product for shortwave radiation (primary pyranometer)
swpp_dp <- 'DP1.00014.001'

## download shortwave radiation data from NEON
neon_download(product = swpp_dp, site = site)

## extract 30 minute shortwave radiation data
rad_table <- neon_read("SRDDP_30min-basic", site = site)

rad_info <- rad_table %>%
  mutate(Date = as_date(endDateTime),
         gloRadMean = if_else(gloRadFinalQF == 0,
                              gloRadMean,
                              NA)) %>%
  group_by(Date) %>%
  summarize(DirRad = mean(gloRadMean, na.rm = TRUE)) %>%
  mutate_if(is.numeric, ~
              replace(., is.nan(.), NA))
rad_info$DirRadInterp <- na_kalman(rad_info$DirRad)

## extract and aggregate radiation data to daily time

rh_dp <- 'DP1.00098.001'
neon_download(product = rh_dp, site = site)
rh_table <- neon_read("RH_30min", site = site)

rh_info <- rh_table %>%
  mutate(Date = as_date(endDateTime),
         rhMean = if_else(RHFinalQF == 0,
                              RHMean,
                              NA)) %>%
  group_by(Date) %>%
  summarize(RelHum = mean(RHMean, na.rm = TRUE)) %>%
  mutate_if(is.numeric, ~
              replace(., is.nan(.), NA))
rh_info$RelHumInterp <- na_kalman(rh_info$RelHum)
```

# 5 What’s next?

-   The forecasts we created here were relatively simple. What are ideas
    you have to improve the forecasts?
-   How could you expand on this forecasting exercise? More fires?
    Different forecast horizons?
-   How could you apply what you have learned here in your research or
    teaching?

# 6 Acknowledgements

We would like to thank everyone who has joined the EFI Cyberinfrastructure calls during the last two years to provide feedback and discuss progress on this project. A special thank you to Carl Boettiger and Quinn Thomas for all of their assistance, from the onset of this project during the 2023 EFI Unconference to present.